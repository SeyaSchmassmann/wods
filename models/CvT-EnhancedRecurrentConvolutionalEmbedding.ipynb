{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPI9Fj_Pes3L"
      },
      "source": [
        "# CvT-Model with Enhanced Convolutional Embedding\n",
        "\n",
        "<img src=\"./../CvT-RecurrentEmbedding.drawio.png?raw=1\" height=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ul2uMlQes3O"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIi1ayzAfHT0"
      },
      "outputs": [],
      "source": [
        "%pip install pytorch-lightning\n",
        "%pip install torch torchvision\n",
        "%pip install lightning\n",
        "%pip install einops\n",
        "%pip install timm\n",
        "%pip install dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq96M21xes3O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "import torch\n",
        "from einops import rearrange\n",
        "import torch.nn as nn\n",
        "from timm.models.layers import DropPath, trunc_normal_\n",
        "\n",
        "IS_PAPERSPACE = os.getcwd().startswith('/notebooks')\n",
        "dir_env = os.path.join(os.getcwd(), '.env') if IS_PAPERSPACE else os.path.join(os.getcwd(), '..', '.env')\n",
        "_ = load_dotenv(dotenv_path=dir_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjIjjCP2es3Q"
      },
      "source": [
        "# Modell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7wK3pcies3Q"
      },
      "outputs": [],
      "source": [
        "class ConvEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size // 2)\n",
        "        self.proj2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=kernel_size // 2)\n",
        "        self.norm = nn.LayerNorm(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('ConvEmbed.forward.0', x.shape)\n",
        "        x = self.proj1(x)\n",
        "        # print('ConvEmbed.forward.0.1', x.shape)\n",
        "        _, _, H, W = x.shape\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        x = self.norm(x)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=int(x.shape[1]**0.5), w=int(x.shape[1]**0.5))\n",
        "        x = self.proj2(x)\n",
        "        # print('ConvEmbed.forward.1', x.shape)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        # print('ConvEmbed.forward.2', x.shape)\n",
        "        x = self.norm(x)\n",
        "        # print('ConvEmbed.forward.3', x.shape)\n",
        "        return x, H, W\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=1, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.proj_q = nn.Linear(dim, dim, bias=False)\n",
        "        self.proj_k = nn.Linear(dim, dim, bias=False)\n",
        "        self.proj_v = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(0.0)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(0.0)\n",
        "\n",
        "        self.drop_path = DropPath(0.1)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.Linear(int(dim * mlp_ratio), dim),\n",
        "            nn.Dropout(0.0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x_norm = self.norm1(x)\n",
        "\n",
        "        q = rearrange(self.proj_q(x_norm), 'b t (h d) -> b h t d', h=self.num_heads)\n",
        "        k = rearrange(self.proj_k(x_norm), 'b t (h d) -> b h t d', h=self.num_heads)\n",
        "        v = rearrange(self.proj_v(x_norm), 'b t (h d) -> b h t d', h=self.num_heads)\n",
        "\n",
        "        attn_score = torch.einsum('bhlk,bhtk->bhlt', [q, k]) * self.scale\n",
        "        attn = nn.functional.softmax(attn_score, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = torch.einsum('bhlt,bhtv->bhlv', [attn, v])\n",
        "        x = rearrange(x, 'b h t d -> b t (h d)')\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        x = residual + self.drop_path(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = residual2 + self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "class CvTStage(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, stride, depth, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed = ConvEmbedding(in_ch, out_ch, kernel_size, stride)\n",
        "        self.dropout = nn.Dropout(0.0)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(out_ch, num_heads) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, H, W = self.embed(x)\n",
        "        x = self.dropout(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        return x, H, W\n",
        "\n",
        "\n",
        "class CvTEnhancedRecurrentConvolutionalEmbedding(nn.Module):\n",
        "    def __init__(self, num_classes=200):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.stage1 = CvTStage(3, 64, kernel_size=5, stride=2, depth=1, num_heads=1)\n",
        "        self.stage2 = CvTStage(64, 192, kernel_size=3, stride=2, depth=2, num_heads=3)\n",
        "        self.stage3 = CvTStage(192, 384, kernel_size=3, stride=1, depth=10, num_heads=6)\n",
        "\n",
        "        self.norm = nn.LayerNorm(384)\n",
        "        self.head = nn.Linear(384, num_classes) \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, H1, W1 = self.stage1(x)\n",
        "        x1_spatial = rearrange(x1, 'b (h w) c -> b c h w', h=H1, w=W1)\n",
        "\n",
        "        x2, H2, W2 = self.stage2(x1_spatial)\n",
        "        x2_spatial = rearrange(x2, 'b (h w) c -> b c h w', h=H2, w=W2)\n",
        "\n",
        "        x3, _, _ = self.stage3(x2_spatial)\n",
        "\n",
        "        x = self.norm(x3)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szfPHjFFes3Q"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy847tS2es3Q"
      },
      "outputs": [],
      "source": [
        "model = CvTEnhancedRecurrentConvolutionalEmbedding()\n",
        "\n",
        "dummy_input = torch.randn(8, 3, 64, 64)\n",
        "output = model(dummy_input)\n",
        "\n",
        "assert output.shape == (8, 200), f\"Expected output shape (8, 200), but got {output.shape}\"\n",
        "print(\"Model output shape is as expected:\", output.shape)\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 64, 64)\n",
        "output = model(dummy_input)\n",
        "\n",
        "assert output.shape == (1, 200), f\"Expected output shape (1, 200), but got {output.shape}\"\n",
        "print(\"Model output shape is as expected:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa41nXqUes3Q"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUFVhRXUes3R"
      },
      "outputs": [],
      "source": [
        "from models.processData import prepare_data_and_get_loaders\n",
        "\n",
        "train_loader, val_loader, test_loader = prepare_data_and_get_loaders(\"/datasets/tiny-imagenet-200/tiny-imagenet-200.zip\", \"data/tiny-imagenet-200\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ssdD8E3es3U"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gfnKuY4es3U"
      },
      "outputs": [],
      "source": [
        "from models.trainModel import train_test_model\n",
        "\n",
        "train_test_model(CvTEnhancedRecurrentConvolutionalEmbedding, train_loader, val_loader, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
