{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPI9Fj_Pes3L"
      },
      "source": [
        "# Original CvT-Model\n",
        "\n",
        "<img src=\"./../CvT-Original.drawio.png?raw=1\" alt=\"CvT-Modell mit Convolutional Embedding\" title=\"CvT-Modell mit Convolutional Embedding\" height=\"400\" />\n",
        "\n",
        "Dimensions sind ohne Batch-Size.\n",
        "\n",
        "## Input-Dimensions\n",
        "\n",
        "**Dimensions:** $H_0 = 64px, \\quad W_0 = 64px, \\quad C_0 = 3$ \\\n",
        "**Output-Shape:** `(3, 64, 64)`\n",
        "\n",
        "## Conv2d\n",
        "\n",
        "Berechnung Output-Dimensions:\n",
        "\n",
        "$ \\text{kernel size}\\ k = 7, \\quad \\text{stride}\\ s = 4, \\quad \\text{padding}\\ p = 3 $ \\\n",
        "$ H_i = \\frac{H_{i-1} + 2p - k}{s}\\ + 1, \\quad W_i = \\frac{W_{i-1} + 2p - k}{s}\\ + 1 $\n",
        "\n",
        "**Output-Dimensions:** $H_1 = 16px, \\quad W_1 = 16px, \\quad C_1 = 64$ \\\n",
        "**Output-Shape:** `(64, 16, 16)`\n",
        "\n",
        "## Flatten\n",
        "\n",
        "**Output-Dimensions:** $H_1 W_1 \\times C_1 = 16*16 \\times 64$ \\\n",
        "**Output-Shape:** `(256, 64)`\n",
        "\n",
        "## Conv Projection\n",
        "\n",
        "\n",
        "\n",
        "## Multi-Head Attention\n",
        "\n",
        "Berechnung der Query-, Key- und Value-Matrizen:\n",
        "\n",
        "$X \\in \\mathbb{R}^{H_1 W_1 \\times C_1}$ \\\n",
        "$d_k$ ist die Dimension der Value-, Query- und Key-Vektoren \\\n",
        "$W^Q, W^K, W^V \\in \\mathbb{R}^{C_1 \\times d_k}$ \\\n",
        "$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$\n",
        "\n",
        "$d_k = 64$ \\\n",
        "$Q, K, V \\in \\mathbb{R}^{256 \\times 64}$\n",
        "\n",
        "**Output-Dimensions:** $256 \\times 64$ \\\n",
        "**Output-Shape:** `(256, 64)`\n",
        "\n",
        "## MLP\n",
        "\n",
        "Expansion factor: $e = 4$\n",
        "\n",
        "1. **Step:** Linear ➔ GELU ➔ Dropout\n",
        "   \n",
        "   **Output-Dimensions:** $256 \\times 64 \\times 4 = 256 \\times 256$ \\\n",
        "   **Output-Shape:** `(256, 256)`\n",
        "\n",
        "2. **Step:** Linear ➔ Dropout\n",
        "\n",
        "    **Output-Dimensions:** $256 \\times 256 \\times 64 = 256 \\times 64$ \\\n",
        "    **Output-Shape:** `(256, 64)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ul2uMlQes3O"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIi1ayzAfHT0"
      },
      "outputs": [],
      "source": [
        "%pip install pytorch-lightning\n",
        "%pip install torch torchvision\n",
        "%pip install lightning\n",
        "%pip install einops\n",
        "%pip install timm\n",
        "%pip install dotenv\n",
        "%pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq96M21xes3O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "IS_PAPERSPACE = os.getcwd().startswith('/notebooks')\n",
        "dir_env = os.path.join(os.getcwd(), '.env') if IS_PAPERSPACE else os.path.join(os.getcwd(), '..', '.env')\n",
        "_ = load_dotenv(dotenv_path=dir_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjIjjCP2es3Q"
      },
      "source": [
        "# Modell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7wK3pcies3Q"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class TinyImageNetCNN(nn.Module):\n",
        "    def __init__(self, num_classes=200):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            ConvBlock(3, 64),\n",
        "            ConvBlock(64, 128),\n",
        "            ConvBlock(128, 256),\n",
        "            ConvBlock(256, 512),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szfPHjFFes3Q"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy847tS2es3Q"
      },
      "outputs": [],
      "source": [
        "model = TinyImageNetCNN()\n",
        "\n",
        "dummy_input = torch.randn(8, 3, 64, 64)\n",
        "output = model(dummy_input)\n",
        "\n",
        "assert output.shape == (8, 200), f\"Expected output shape (8, 200), but got {output.shape}\"\n",
        "print(\"Model output shape is as expected:\", output.shape)\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 64, 64)\n",
        "output = model(dummy_input)\n",
        "\n",
        "assert output.shape == (1, 200), f\"Expected output shape (1, 200), but got {output.shape}\"\n",
        "print(\"Model output shape is as expected:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa41nXqUes3Q"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUFVhRXUes3R"
      },
      "outputs": [],
      "source": [
        "from models.processData import prepare_data_and_get_loaders\n",
        "\n",
        "train_loader, val_loader, test_loader = prepare_data_and_get_loaders(\"/datasets/tiny-imagenet-200/tiny-imagenet-200.zip\", \"data/tiny-imagenet-200\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1d7soVVes3S"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D5wTcwres3S"
      },
      "outputs": [],
      "source": [
        "# def imshow(img):\n",
        "#     img = img / 2 + 0.5\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.axis('off')\n",
        "#     plt.show()\n",
        "# \n",
        "# image, label = train_loader.dataset[0]\n",
        "# imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ssdD8E3es3U"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gfnKuY4es3U"
      },
      "outputs": [],
      "source": [
        "from models.trainModel import train_test_model\n",
        "\n",
        "train_test_model(TinyImageNetCNN, train_loader, val_loader, test_loader, 30)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
